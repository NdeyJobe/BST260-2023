[
  {
    "objectID": "20-regression.html#case-study-is-height-hereditary",
    "href": "20-regression.html#case-study-is-height-hereditary",
    "title": "20  Regression",
    "section": "20.1 Case study: is height hereditary?",
    "text": "20.1 Case study: is height hereditary?\n\nlibrary(tidyverse)\nlibrary(HistData)\n\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\n\ngalton_heights |&gt; \n  summarize(mean(father), sd(father), mean(son), sd(son))\n\n# A tibble: 1 × 4\n  `mean(father)` `sd(father)` `mean(son)` `sd(son)`\n           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1           69.1         2.55        69.2      2.71\n\n\nHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\n\ngalton_heights |&gt; ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "20-regression.html#sec-corr-coef",
    "href": "20-regression.html#sec-corr-coef",
    "title": "20  Regression",
    "section": "20.2 The correlation coefficient",
    "text": "20.2 The correlation coefficient\nThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\n\\]\n\nrho &lt;- mean(scale(x) * scale(y))\n\nLet’s see why this makes sense\n\n\n\n\n\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[\n\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\n\\frac{1}{\\sigma_x^2} \\sigma^2_x =\n1\n\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation, computed with the function cor, between father and son’s heights is about 0.5:\n\ngalton_heights |&gt; summarize(r = cor(father, son)) |&gt; pull(r)\n\n[1] 0.4334102\n\n\n\nFor reasons similar to those explained in Section Section 19.2.3 for the standard deviation, cor(x,y) divides by length(x)-1 rather than length(x).\n\nTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\n\n\n\n\n\n\n20.2.1 Sample correlation is a random variable\nLet’s consider our Galton heights to be the population and take random samples of size 25:\n\nr &lt;- sample_n(galton_heights, 25, replace = TRUE) |&gt; \n  summarize(r = cor(father, son)) |&gt; pull(r)\n\n`r`` is a random variable. We can run a Monte Carlo simulation to see its distribution:\n\nB &lt;- 1000\nN &lt;- 25\nr &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; \n    pull(r)\n})\nhist(r, breaks = 20)\n\n\n\n\nWe see that the expected value of `r`` is the population correlation:\n\nmean(r)\n\n[1] 0.4316779\n\n\nand that it has a relatively high standard error relative to the range of values R can take:\n\nsd(r)\n\n[1] 0.1652205\n\n\nSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nDoes CLT apply?\nThe SE can be shown to be \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\n\nggplot(aes(sample = r), data = data.frame(r)) + \n  stat_qq() + \n  geom_abline(intercept = mean(r), slope = sqrt((1 - mean(r)^2)/(N - 2)))\n\n\n\n\nIf you increase \\(N\\), you will see the distribution converging to normal.\n\nB &lt;- 1000\nN &lt;- 50\nr &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    summarize(r = cor(father, son)) |&gt; \n    pull(r)\n})\nggplot(aes(sample = r), data = data.frame(r)) + \n  stat_qq() + \n  geom_abline(intercept = mean(r), slope = sqrt((1 - mean(r)^2)/(N - 2)))\n\n\n\n\n\n\n20.2.2 Correlation is not always a useful summary\nCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere are some other fun examples:\n\nlibrary(datasauRus)\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~dataset)\n\n\n\n\nCorrelation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction."
  },
  {
    "objectID": "20-regression.html#sec-conditional-expectation",
    "href": "20-regression.html#sec-conditional-expectation",
    "title": "20  Regression",
    "section": "20.3 Conditional expectations",
    "text": "20.3 Conditional expectations\nSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. How do we do this?\nMathematically, we can show that the conditional expectation\n\\[\nf(x) = \\mbox{E}(Y \\mid X = x)\n\\]\nminimizes the expected squared error (MSE):\n\\[\n\\mbox{E}[\\{Y -f(X)\\}^2]\n\\] with \\(x\\) representing the fixed value that defines that subset, for example 72 inches.\nWe denote the standard deviation of the strata with\n\\[\n\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\n\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nHowever, we oftern have a limited number of points to estimate this conditional expectation. For example\n\nsum(galton_heights$father == 72)\n\n[1] 8\n\n\nfathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\n\nsum(galton_heights$father == 72.5)\n\n[1] 1\n\n\nA practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 72) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n\n[1] 70.5\n\n\nNote that a 72-inch father is taller than average but smaller than 72. We see the same for someoen slightly shorted than average:\n\nconditional_avg &lt;- galton_heights |&gt; \n  filter(round(father) == 67) |&gt;\n  summarize(avg = mean(son)) |&gt; \n  pull(avg)\nconditional_avg\n\n[1] 68.71818\n\n\nThe sons of have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later section, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\n\ngalton_heights |&gt; mutate(father_strata = factor(round(father))) |&gt; \n  ggplot(aes(father_strata, son)) + \n  geom_boxplot() + \n  geom_point()\n\n\n\n\nNot surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n\n\n\n\n\nThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line."
  },
  {
    "objectID": "20-regression.html#the-regression-line",
    "href": "20-regression.html#the-regression-line",
    "title": "20  Regression",
    "section": "20.4 The regression line",
    "text": "20.4 The regression line\nIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), our prediction \\(\\hat{Y}\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\n\\left( \\frac{\\hat{Y}-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\n\\]\nWe can rewrite it like this:\n\\[\n\\hat{Y} = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\n\\]\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\n\\hat{Y} = b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\n\\]\nHere we add the regression line to the original data:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) +\n  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) \n\n\n\n\nThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\n\ngalton_heights |&gt; \n  ggplot(aes(scale(father), scale(son))) + \n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = r)"
  },
  {
    "objectID": "20-regression.html#regression-improves-precision",
    "href": "20-regression.html#regression-improves-precision",
    "title": "20  Regression",
    "section": "20.5 Regression improves precision",
    "text": "20.5 Regression improves precision\nLet’s compare the two approaches to prediction that we have presented:\n\nRound fathers’ heights to closest inch, stratify, and then take the average.\nCompute the regression line and use it to predict.\n\nWe use a Monte Carlo simulation sampling \\(N=50\\) families:\n\nB &lt;- 1000\nN &lt;- 50\n\nset.seed(1983)\nconditional_avg &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  dat |&gt; filter(round(father) == 72) |&gt; \n    summarize(avg = mean(son)) |&gt; \n    pull(avg)\n  })\n\nregression_prediction &lt;- replicate(B, {\n  dat &lt;- sample_n(galton_heights, N)\n  mu_x &lt;- mean(dat$father)\n  mu_y &lt;- mean(dat$son)\n  s_x &lt;- sd(dat$father)\n  s_y &lt;- sd(dat$son)\n  r &lt;- cor(dat$father, dat$son)\n  mu_y + r*(72 - mu_x)/s_x*s_y\n})\n\nAlthough the expected value of these two random variables is about the same:\n\nmean(conditional_avg, na.rm = TRUE)\n\n[1] 70.49368\n\nmean(regression_prediction)\n\n[1] 70.50941\n\n\nThe standard error for the regression prediction is substantially smaller:\n\nsd(conditional_avg, na.rm = TRUE)\n\n[1] 0.9635814\n\nsd(regression_prediction)\n\n[1] 0.4520833\n\n\nThe regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data."
  },
  {
    "objectID": "20-regression.html#bivariate-normal-distribution",
    "href": "20-regression.html#bivariate-normal-distribution",
    "title": "20  Regression",
    "section": "20.6 Bivariate normal distribution",
    "text": "20.6 Bivariate normal distribution\nCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. As we saw in Section Section 20.2), they can be thin (high correlation) or circle-shaped (no correlation.\nA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal. When three or more variables have the property that each pair is bivariate normal, we say the variables follow a multivariate normal distribution or that they are jointly normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\n\ngalton_heights |&gt;\n  mutate(z_father = round((father - mean(father)) / sd(father))) |&gt;\n  filter(z_father %in% -2:2) |&gt;\n  ggplot() +  \n  stat_qq(aes(sample = son)) +\n  facet_wrap( ~ z_father) \n\n\n\n\nNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\n\\mbox{E}(Y | X=x) = \\mu_Y +  \\rho \\frac{x-\\mu_X}{\\sigma_X}\\sigma_Y\n\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\n\\frac{\\mbox{E}(Y \\mid X=x)  - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line."
  },
  {
    "objectID": "20-regression.html#variance-explained",
    "href": "20-regression.html#variance-explained",
    "title": "20  Regression",
    "section": "20.7 Variance explained",
    "text": "20.7 Variance explained\nThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\n\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\n\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution."
  },
  {
    "objectID": "20-regression.html#there-are-two-regression-lines",
    "href": "20-regression.html#there-are-two-regression-lines",
    "title": "20  Regression",
    "section": "20.8 There are two regression lines",
    "text": "20.8 There are two regression lines\nWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\n\nmu_x &lt;- mean(galton_heights$father)\nmu_y &lt;- mean(galton_heights$son)\ns_x &lt;- sd(galton_heights$father)\ns_y &lt;- sd(galton_heights$son)\nr &lt;- cor(galton_heights$father, galton_heights$son)\nm_1 &lt;-  r * s_y / s_x\nb_1 &lt;- mu_y - m_1*mu_x\n\nwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\n\nm_2 &lt;-  r * s_x / s_y\nb_2 &lt;- mu_x - m_2 * mu_y\n\nSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\n\ngalton_heights |&gt; \n  ggplot(aes(father, son)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = b_1, slope = m_1, col = \"blue\") +\n  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \"red\")"
  },
  {
    "objectID": "20-regression.html#linear-models",
    "href": "20-regression.html#linear-models",
    "title": "20  Regression",
    "section": "20.9 Linear models",
    "text": "20.9 Linear models\nLet’s learn about the connection between regression and linear models. We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nlinear here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example,\n\\[3x - 4y + 5z\\]\nis a linear combination of \\(x\\), \\(y\\), and \\(z\\).\nWe can also add a constant so\n\\[2 + 3x - 4y + 5z\\]\nis also linear combination of \\(x\\), \\(y\\), and \\(z\\).\nNote that if we write\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon\n\\]\nthen if we assume \\(\\varepsilon\\) follows a normal distribution with expected value 0 and fixed standard deviation, then \\(Y\\) has the same properties as the regression setup gave us: it follows a normal distribution, the expected value is a linear function \\(x\\), and the standard deviation does not depend on \\(x\\).\n\n\n\n\n\n\nNote\n\n\n\nIn statistical textbooks, the \\(\\varepsilon\\)s are referred to as “errors,” which originally represented measurement errors in the initial applications of these models. These errors were associated with inaccuracies in measuring height, weight, or distance. However, the term “error” is now used more broadly, even when the \\(\\varepsilon\\)s do not necessarily signify an actual error. For instance, in the case of height, if someone is 2 inches taller than expected based on their parents’ height, those 2 inches should not be considered an error. Despite its lack of descriptive accuracy, the term “error” is employed to elucidate the unexplained variability in the model, unrelated to other included terms.\n\n\nLinear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N.\n\\]\nWe can further assume that \\(\\varepsilon_i\\) are independent from each other and all have the same standard deviation.\nTo have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data.\nAlthough this model is exactly the same one we derived earlier by assuming bivariate normal data, a somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not necessarily specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nOne reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[\nY_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N\n\\]\nIn this case \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father."
  },
  {
    "objectID": "20-regression.html#sec-lse",
    "href": "20-regression.html#sec-lse",
    "title": "20  Regression",
    "section": "20.10 Least Squares Estimates",
    "text": "20.10 Least Squares Estimates\nThe standard approach to estimate the \\(\\beta\\)s is to find the values that minimize the distance of the fitted model to the data:\n\\[\nRSS = \\sum_{i=1}^n \\left\\{  y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2\n\\]\nThis quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Let’s demonstrate this with the previously defined dataset:\n\nlibrary(HistData)\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\n\nLet’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\n\nrss &lt;- function(beta0, beta1, data){\n  resid &lt;- galton_heights$son - (beta0 + beta1*galton_heights$father)\n  return(sum(resid^2))\n}\n\nSo for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25.\n\nbeta1 = seq(0, 1, length = nrow(galton_heights))\nresults &lt;- data.frame(beta1 = beta1,\n                      rss = sapply(beta1, rss, beta0 = 25))\nresults |&gt; ggplot(aes(beta1, rss)) + geom_line() + \n  geom_line(aes(beta1, rss))\n\n\n\n\nWe can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs. We don’t need trial and error because we can use calculus."
  },
  {
    "objectID": "20-regression.html#the-lm-function",
    "href": "20-regression.html#the-lm-function",
    "title": "20  Regression",
    "section": "20.11 The lm function",
    "text": "20.11 The lm function\nIn R, we can obtain the least squares estimates using the lm function. To fit the model:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nwith \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height, we can use this code to obtain the least squares estimates.\n\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coef\n\n(Intercept)      father \n  37.287605    0.461392 \n\n\nThe most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit.\nThe object fit includes more information about the fit. We can use the function summary to extract more of this information (not shown):\n\nsummary(fit)\n\n\nCall:\nlm(formula = son ~ father, data = galton_heights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3543 -1.5657 -0.0078  1.7263  9.4150 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.28761    4.98618   7.478 3.37e-12 ***\nfather       0.46139    0.07211   6.398 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 177 degrees of freedom\nMultiple R-squared:  0.1878,    Adjusted R-squared:  0.1833 \nF-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09\n\n\nTo understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables."
  },
  {
    "objectID": "20-regression.html#lse-are-random-variables",
    "href": "20-regression.html#lse-are-random-variables",
    "title": "20  Regression",
    "section": "20.12 LSE are random variables",
    "text": "20.12 LSE are random variables\nThe LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. Here is a Monte Carlo simulation demonstrating it:\n\nB &lt;- 1000\nN &lt;- 50\nlse &lt;- replicate(B, {\n  sample_n(galton_heights, N, replace = TRUE) |&gt; \n    lm(son ~ father, data = _) |&gt; \n    coef()\n})\nlse &lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) \n\nWe can see the variability of the estimates by plotting their distributions:\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\n\n\nThe reason these look normal is because the central limit theorem applies here as well. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. Here it is for one of our simulated data sets:\n\nsample_n(galton_heights, N, replace = TRUE) |&gt; \n  lm(son ~ father, data = _) |&gt; \n  summary() |&gt; \n  coef()\n\n              Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 19.2791952 11.6564590 1.653950 0.1046637693\nfather       0.7198756  0.1693834 4.249977 0.0000979167\n\n\nYou can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation:\n\nlse |&gt; summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))\n\n     se_0      se_1\n1 8.83591 0.1278812\n\n\nThe summary function also reports t-statistics (t value) and p-values (Pr(&gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\n\n\n\n\n\n\nWarning\n\n\n\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true."
  },
  {
    "objectID": "20-regression.html#predicted-values-are-random-variables",
    "href": "20-regression.html#predicted-values-are-random-variables",
    "title": "20  Regression",
    "section": "20.13 Predicted values are random variables",
    "text": "20.13 Predicted values are random variables\nOnce we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nThe ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\n\ngalton_heights |&gt; ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\n\nfit &lt;- galton_heights |&gt; lm(son ~ father, data = _) \n\ny_hat &lt;- predict(fit, se.fit = TRUE)\n\nnames(y_hat)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\""
  },
  {
    "objectID": "20-regression.html#diagnostic-plots",
    "href": "20-regression.html#diagnostic-plots",
    "title": "20  Regression",
    "section": "20.14 Diagnostic plots",
    "text": "20.14 Diagnostic plots\nWhen the linear model is assumed rather than derived, all interpretations depend on the usefulness of the model. The lm function will fit the model and return summaries even when the model is wrong and unuseful.\nVisually inspecting residuals, defined as the difference between observed values and predicted values\n\\[\nr = Y - \\hat{Y} = Y - \\left(\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right),\n\\]\nand summaries of the residuals, is a powerful way to diagnose if the model is useful. Note that the residuals can be thought of estimates of the errors since\n\\[\n\\varepsilon = Y - \\left(\\beta_0 + \\beta_1 x_i \\right).\n\\]\nIn fact, residuals are often denoted as \\(\\hat{\\varepsilon}\\). This motivates several diagnostic plots. Becasue we obervere, \\(r\\) but don’t observe \\(\\varepsilon\\), we based the plots on the residuals.\n\nBecause the errors are assumed not to depend on the expected value of \\(Y\\), a plot of \\(r\\) versus the fitted values \\(\\hat{Y}\\) should show no relationship.\nIn cases in which we assume the errors follow a normal distribtuion a qqplot of standardized \\(r\\) should fall on a line when plotted against theoretical quantiles.\nBecause we assume the standard deviation of the errors is constant, if we plot the absolute value of the residuals, it should appear constant.\n\nWe prefer plots rather than summaries based on, for example, correlation because, as noted in Section @ascombe, correlation is not always the best summary of association. The function plot applied to an lm object automatically plots these.\n\nplot(fit, which = 1:3)\n\nThis function can produce six different plots, and the argument which let’s you specify which you want to see. You can learn more by reading the plot.lm help file."
  },
  {
    "objectID": "20-regression.html#the-regression-fallacy",
    "href": "20-regression.html#the-regression-fallacy",
    "title": "20  Regression",
    "section": "20.15 The regression fallacy",
    "text": "20.15 The regression fallacy\nWikipedia defines the sophomore slump as:\n\nA sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n\nIn Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for widely used measure of success, the batting average, we see that this observation holds true for the top performing ROYs:\n\n\n\n\n\nnameFirst\nnameLast\nrookie_year\nrookie\nsophomore\n\n\n\n\nWillie\nMcCovey\n1959\n0.3541667\n0.2384615\n\n\nIchiro\nSuzuki\n2001\n0.3497110\n0.3214838\n\n\nAl\nBumbry\n1973\n0.3370787\n0.2333333\n\n\nFred\nLynn\n1975\n0.3314394\n0.3136095\n\n\nAlbert\nPujols\n2001\n0.3288136\n0.3135593\n\n\n\n\n\n\n\nIn fact, the proportion of players that have a lower batting average their sophomore year is 0.6981132.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\n\n\n`summarise()` has grouped output by 'playerID'. You can override using the\n`.groups` argument.\n\n\nThe same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nMiguel\nCabrera\n0.3477477\n0.3126023\n\n\nHanley\nRamirez\n0.3453947\n0.2828508\n\n\nMichael\nCuddyer\n0.3312883\n0.3315789\n\n\nScooter\nGennett\n0.3239437\n0.2886364\n\n\nJoe\nMauer\n0.3235955\n0.2769231\n\n\n\n\n\n\n\nBut these are not rookies! Also, look at what happens to the worst performers of 2013:\n\n\n\n\n\nnameFirst\nnameLast\n2013\n2014\n\n\n\n\nDanny\nEspinosa\n0.1582278\n0.2192192\n\n\nDan\nUggla\n0.1785714\n0.1489362\n\n\nJeff\nMathis\n0.1810345\n0.2000000\n\n\nB. J.\nUpton\n0.1841432\n0.2080925\n\n\nAdam\nRosales\n0.1904762\n0.2621951\n\n\n\n\n\n\n\nTheir batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\n\n\n\n\n\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean."
  },
  {
    "objectID": "20-regression.html#exercises",
    "href": "20-regression.html#exercises",
    "title": "20  Regression",
    "section": "20.16 Exercises",
    "text": "20.16 Exercises\n1. Load the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n2. Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n3. Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons."
  },
  {
    "objectID": "20-regression.html#footnotes",
    "href": "20-regression.html#footnotes",
    "title": "20  Regression",
    "section": "",
    "text": "http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715↩︎"
  },
  {
    "objectID": "pset2.html#introduction",
    "href": "pset2.html#introduction",
    "title": "Problem Set 2",
    "section": "Introduction",
    "text": "Introduction\nFor this assignment, you’ll delve into data wrangling, statistical inference, and linear modeling that was used by academics to gain a deeper understanding of the efforts made to estimate the indirect death toll in Puerto Rico following Hurricane María. Begin by reviewing this comprehensive timeline and summary. Initially, we’ll use data wrangling techniques to extract information from documents released by organizations that had early access to the mortality registry data. Following that, we’ll work with the mortality registry data that has since been publicly disclosed by the government. To determine mortality rates, it’s essential to acquire data on population size, categorized by age and sex. We’ll achieve this by utilizing APIs provided by the US Census.\nThese are the libraries you will need and the only ones you are allowed to load\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(pdftools)\nlibrary(janitor)\nlibrary(httr2)\nlibrary(excessmort)\nlibrary(jsonlite)\nlibrary(purrr)\n\nYou don’t need these but we will allow you to load them:\n\nlibrary(ggthemes)\nlibrary(ThemePark)\nlibrary(ggrepel)\n\nReminders:\n\nAdd a title to all your graphs.\nAdd a label to the x and y axes when not obvious what they are showing.\nThink about transformations that convey the message in clearer fashion."
  },
  {
    "objectID": "pset2.html#preparation",
    "href": "pset2.html#preparation",
    "title": "Problem Set 2",
    "section": "Preparation",
    "text": "Preparation\nCreate a directory for this homework. In this directory create two subdirectories: data and rdas. You will also create a get-population.R file where you will have the code to download and wrangle population data from the US Census."
  },
  {
    "objectID": "pset2.html#wrangling",
    "href": "pset2.html#wrangling",
    "title": "Problem Set 2",
    "section": "Wrangling",
    "text": "Wrangling\n\nIn December 2017 a preprint was published that includes data from the mortality registry. It is a Word document that you can download from https://osf.io/preprints/socarxiv/s7dmu/download. Save a PDF copy of this document to your data directory.\nRead in the PFD file into R and create a data frame with the data in Table 1 of the paper. The data frame should be tidy with columns months, year, and deaths. Your data frame need not include the confidence intervals or averages.\nFor each month compute the average and a 95% confidence interval to reproduce Figure 3 in the preprint. Make sure to show the month names on the x-axis, not numbers. Hint: Save the graph to an object to make an upcoming exercise easier.\nThe model here seems to be that the observed death for month \\(i\\) and year \\(j\\) is\n\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}\n\\]\nwith \\(\\text{Var}(\\varepsilon_{ij}) = \\sigma^2_i\\). The preprint reports the September and October 2017 deaths as 2,987 and 3,043. Create a data frame called dat_2017 with these two values and include an estimate for the standard error of this random variable. Hint: Look at the model and use data from 2010-2016 to estimate \\(\\sigma_i\\).\n\nMake a plot now that includes the two points for 2017 and the 1.96 standard errors bars around them. Are the deaths statistically significantly different than the expected based on 2010-2016 data?\nOn December 8, 2017 the New York Times publishes an article with daily counts. They share the data that was provided to them by the Mortality Registry. It is PDF you can obtain here. Read the PDF into R and extract the daily counts. Save the results to a data frame called dat with columns data and deaths. Make sure the data frame is ordered by date.\nPlot the deaths versus dates and describe what you see towards the end for 2017.\nThe reason you see a drop at the end is because it takes time to officially register deaths. It takes about 45 days for 99% of the data to be added. Remove the last 45 days and remake the plot, but this time showing deaths against day of the year (1 through 365 or 366) with color highlighting what happened after the hurricane. Do not include a legend."
  },
  {
    "objectID": "pset2.html#us-census-apis",
    "href": "pset2.html#us-census-apis",
    "title": "Problem Set 2",
    "section": "US Census APIs",
    "text": "US Census APIs\nIn June 2018, data was finally made public. This dataset gives you deaths by age group and sex obtained more recently from the Mortality Registry. In preparation for the analysis of these data, we will obtain population estimates from the US Census by age and gender.\nWe will be using three different APIs as that is how the Census makes the data available. Important to note that in two of these APIs, all ages 85 or above are grouped into one group.\nIf you wish to skip this section (though you will lose points), you can obtain the already wrangled population data here.\n\nFirst step is to obtain a census key. You can request one here https://api.census.gov/data/key_signup.html. Once you have a key create a file in your directory called census-key.R that simply defines the variable census_key to be your personal key. Do not share this key publicly. The quarto file you turn in should not show your census key, instead it should source a file called census-key.R to define the variable. We will have a file on our end with our key so your script can knit.\nOnce you have your key you can use the httr2 package to download the data directly from the Census data base. We will start downloading the intercensus data from 2000-2009 (data dictionary here). We will download it only for Puerto Rico which has region ID 72. The following code downloads the data.\n\n\nurl &lt;- \"https://api.census.gov/data/2000/pep\"\nsource(\"census-key.R\")\nendpoint &lt;- paste0(\"int_charage?get=POP,SEX,AGE,DATE_&for=state:72&key=\", census_key)\nresponse &lt;- request(url) |&gt; \n  req_url_path_append(endpoint) |&gt;\n  req_perform()  \n\nThe data is now included in response and you can access it using the resp functions in httr2. Examine the results you obtain when applying resp_body_string. Write code to convert this into a data frame with columns names year, sex, age, and population and call it pop1. Hint: Use the function fromJSON from the jsonlite package. The functions row_to_names and clean_names from the janitor package might also be handy. Use the codebook to understand how the date column relates to year.\n\nNow we will obtain data for 2010-2019. The intercensal data is not available so we will use Vintage 2019 data (data dictionary here). We can follow a similar procedure but with the following API and endpoints:\n\n\nurl &lt;- \"https://api.census.gov/data/2019/pep\"\nsource(\"census-key.R\")\nendpoint &lt;- paste0(\"charage?get=POP,SEX,AGE,DATE_CODE&for=state:72&key=\", census_key)\n\nDownload the data and write code to convert this into a data frame with columns names year, sex, age, and population and call it pop2.\n\nCombine the data frames pop1 and pop2 created in the previous exercises to form one population data frame called population and including all year. Make sure the 85+ category is correctly computed on the two datasets. Save it to a file called population.rds in your rds."
  },
  {
    "objectID": "pset2.html#daily-count-data",
    "href": "pset2.html#daily-count-data",
    "title": "Problem Set 2",
    "section": "Daily count data",
    "text": "Daily count data\nLet’s repeat the analysis done in the preprint but now using 2002-2016 data and, to better see the effect of the hurricane, let’s use weekly instead of monthly and start our weeks on the day the hurricane hit.\nYou can load the data from the excessmort package.\n\ndata(\"puerto_rico_counts\")\n\n\nDefine an object counts by wrangling puerto_rico_counts to 1) include data only from 2002-2017, 2) remove the population column, and 3) to match our population, combine the counts for those 85 and older together.\nCollapse the population data so that it combines agegroups like counts. Also change the sex column so that it matches counts as well.\nAdd a population column to counts using the population data frame you just created.\nUse R to determine what day of the week did María make landfall in PR.\nRedefine the date column to be the start of the week that day is part of. Use the day of the week María made landfall as the first day. Now collapse the data frame to weekly data by redefining outcome to have the total deaths that week for each sex and agegroup. Remove weeks that have less the 7 days. Finally, add a column with the MMWR week. Name the resulting data frame weekly_counts\nMake a per-week version of the plot we made for monthly totals. Make a boxplot for each week based on the 2002-2016 data, then add red points for 2017. Comment on the possibility that indirect effect went past October.\nIf we look at 2017 data before September and compare each week to the average from 2002-2016. What percent are below the median?\nWhy are 2017 totals somewhat below-average? Plot the population in millions against date. What do you see?\nWhen comparing mortalisty across populations of different sizes, we need to look at rates not totals. Because the population is decreasing, this is particularly important. Redo the boxplots but for rates instead of totals.\nNow the rates are all way above average! What is going on? Compute and plot the population sizes against year for each sex of the following age groups: 0-19, 20-39, 40-59, 60+. Describe what you see in this plot then explain why 2017 has higher average death rates.\nCompute the death rates (deaths per 1,000 per year) by the agegroups for each year 2002-2016. Use a transformation of the y-axis that permits us to see the data clearly. Make a separate plot for males and females. Describe in two sentences what you learn.\nRepeat the above but use facet_wrap with scales = \"free_y\" to get a closer look at the patterns for each age group. In this case use color to distinguish the sexes. Describe the pattern observed for the death rate over time."
  },
  {
    "objectID": "pset2.html#linear-models",
    "href": "pset2.html#linear-models",
    "title": "Problem Set 2",
    "section": "Linear models",
    "text": "Linear models\n\nWe are going fit a linear model to account for the trend in death rates to obtain an more appropriate expected death rate for each agegroup and sex. Because we are fitting a linear model, it is preferable to have normally distributed data. We want the number of deaths per week to be larger than 10 for each group. Compute the average number of deaths per week by agegroup and sex for 2016. Based on these data, what agegroups do you recommend we combine?\nCreate a new dataset called dat that collapses the counts into agegroups with enough deaths to fit a linear model. Remove any week with MMWR week 53 and add a column t that includes the number of weeks since the first week in the first year.\nWrite a function that receives a data frame tab, fits a linear model with a line for the time trend, and returns a data frame with 2017 data including a prediction.\nUse the group_modify function to fit this model to each sex and agegroup. Save the results in res.\nFor agegroup and by sex, plot the expected counts for each week with an error bar showing two standard deviations and in red the observed counts. Does the model appear to fit? Hint: Look to see if the red dots are inside the intervals before the hurricane.\nNow estimate weekly excess deaths for 2017 based on the rates esimated from 2002-2016 but the population sizes of 2017. Compare this to estimated standard deviation observed from year to year once we account for trends.\nPlot cummulative excess death for 2017 including a standard error."
  }
]