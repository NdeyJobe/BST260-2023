[
  {
    "objectID": "29-regularization.html#sec-recommendation-systems",
    "href": "29-regularization.html#sec-recommendation-systems",
    "title": "29  Regularization",
    "section": "29.1 Case study: recommendation systems",
    "text": "29.1 Case study: recommendation systems\n\nDuring its initial years of operation, Netflix used a 5-star recommendation system.\nOne star suggests it is not a good movie, whereas five stars suggests it is an excellent movie.\nHere, we provide the basics of how these recommendations are made, motivated by some of the approaches taken by the winners of the Netflix challenges.\nIn October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars.\nIn September 2009, the winners were announced1.\nYou can read a summary of how the winning algorithm was put together here: http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/\nand a more detailed explanation here: https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf.\nWe will now show you some of the data analysis strategies used by the winning team.\n\n\n29.1.1 Movielens data\n\nThe Netflix data is not publicly available, but the GroupLens research lab2 generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users.\nWe make a small subset of this data available via the dslabs package:\n\n\n\n# A tibble: 5 × 7\n  movieId title                              year genres userId rating timestamp\n    &lt;int&gt; &lt;chr&gt;                             &lt;int&gt; &lt;fct&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;\n1      31 Dangerous Minds                    1995 Drama       1    2.5    1.26e9\n2    1029 Dumbo                              1941 Anima…      1    3      1.26e9\n3    1061 Sleepers                           1996 Thril…      1    3      1.26e9\n4    1129 Escape from New York               1981 Actio…      1    2      1.26e9\n5    1172 Cinema Paradiso (Nuovo cinema Pa…  1989 Drama       1    4      1.26e9\n\n\n\nEach row represents a rating given by one user to one movie.\nWe can see the number of unique users that provided ratings and how many unique movies were rated:\n\n\nmovielens |&gt; summarize(n_distinct(userId), n_distinct(movieId))\n\n  n_distinct(userId) n_distinct(movieId)\n1                671                9066\n\n\n\nNot every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells.\nHere is the matrix for six users and four movies.\n\n\n\n\n\n\nuserId\nPulp Fiction\nShawshank Redemption\nForrest Gump\nSilence of the Lambs\n\n\n\n\n13\n3.5\n4.5\n5.0\nNA\n\n\n15\n5.0\n2.0\n1.0\n5.0\n\n\n16\nNA\n4.0\nNA\nNA\n\n\n17\n5.0\n5.0\n2.5\n4.5\n\n\n19\n5.0\n4.0\n5.0\n3.0\n\n\n20\n0.5\n4.5\n2.0\n0.5\n\n\n\n\n\n\n\n\nYou can think of the task of a recommendation system as filling in the NAs in the table above.\nTo see how sparse the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.\n\n\n\n\n\n\n\nLet’s look at some of the general properties of the data to better understand the challenges.\nHere is the distribution of number of ratins for each movie.\n\n\n\n\n\n\n\nWe need to build an algorithm with the collected data that will then be applied outside our control, as users look for movie recommendations.\nSo let’s create a test set to assess the accuracy of the models we implement.\nWe then split the data into a training set and test set by assigning 20% of the ratings made by each user to the test set:\n\n\nset.seed(2006)\nindexes &lt;- split(1:nrow(movielens), movielens$userId)\ntest_ind &lt;- sapply(indexes, function(i) sample(i, ceiling(length(i)*.2))) |&gt; \n  unlist() |&gt;\n  sort()\ntest_set &lt;- movielens[test_ind,]\ntrain_set &lt;- movielens[-test_ind,]\n\n\nTo make sure we don’t include movies that are not in both test and train sets, we remove entries using the semi_join function:\n\n\ntest_set &lt;- test_set |&gt; semi_join(train_set, by = \"movieId\")\ntrain_set &lt;- train_set |&gt; semi_join(test_set, by = \"movieId\")\n\n\nWe use pivot_wider to make a matrix with users represented by rows and movies by the columns\n\n\ny_train &lt;- select(train_set, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) |&gt;\n  column_to_rownames(\"userId\") |&gt;\n  as.matrix()\n\ny_test &lt;- select(test_set, movieId, userId, rating) |&gt;\n  pivot_wider(names_from = movieId, values_from = rating) |&gt;\n  column_to_rownames(\"userId\") |&gt;\n  as.matrix() \n\ny_test &lt;- y_test[rownames(y_train), colnames(y_train)]\n\n\nFinally, we create table to map movie ids to titles:\n\n\nmovie_map &lt;- train_set |&gt; select(movieId, title) |&gt; distinct(movieId, .keep_all = TRUE)"
  },
  {
    "objectID": "29-regularization.html#sec-netflix-loss-function",
    "href": "29-regularization.html#sec-netflix-loss-function",
    "title": "29  Regularization",
    "section": "29.2 Loss function",
    "text": "29.2 Loss function\n\nThe Netflix challenge decided on a winner based on the root mean squared error (RMSE) computed on the test set. We define \\(y_{u,i}\\) as the rating for movie \\(i\\) by user \\(u\\) in the test set and denote the prediction, obtained from the training set, with \\(\\hat{y}_{u,i}\\). We then define the residuals as:\n\n\\[\nr_{u,i} = y_{u,i} - \\hat{y}_{u,i}\n\\]\n\nThe metric used by the competion was defined as:\n\n\\[\n\\mbox{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{u,i}^{} r_{u,i}^2 }\n\\]\n\nwith \\(N\\) being the number of user/movie combinations for which we made predictions and the sum occurring over all these combinations.\nWe can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating.\nIf this number is larger than 1, it means our typical error is larger than one star, which is not good.\nWe define a function to compute this quantity for any set of residuals:\n\n\nrmse &lt;- function(r) sqrt(mean(r^2, na.rm = TRUE))\n\n\nIn this chapter and the next we introduce two concepts, regularization and matrix factorization, that were used by the winners of the Netflix challenge to obtain lowest RMSE.\n\n\n\n\n\n\n\nNote\n\n\n\nWe later provide a formal discussion of the mean squared error."
  },
  {
    "objectID": "29-regularization.html#a-first-model",
    "href": "29-regularization.html#a-first-model",
    "title": "29  Regularization",
    "section": "29.3 A first model",
    "text": "29.3 A first model\n\nFirst model\n\n\\[\nY_{u,i} = \\mu + \\varepsilon_{u,i}\n\\]\nwith \\(\\varepsilon_{i,u}\\) independent errors sampled from the same distribution centered at 0 and \\(\\mu\\) the true rating for all movies.\n\nWe know that the estimate that minimizes the RMSE is the least squares estimate of \\(\\mu\\) and, in this case, is the average of all ratings:\n\n\nmu &lt;- mean(y_train, na.rm = TRUE)\nmu\n\n[1] 3.578109\n\n\n\nIf we predict all unknown ratings with \\(\\hat{\\mu}\\) we obtain the following RMSE:\n\n\nrmse(y_test - mu)\n\n[1] 1.050776\n\n\n\nKeep in mind that if you plug in any other number, you get a higher RMSE. For example:\n\n\nrmse(y_test - 3)\n\n[1] 1.191159\n\n\n\nFrom looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution.\nWe get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857.\nSo we can definitely do better!"
  },
  {
    "objectID": "29-regularization.html#modeling-movie-effects",
    "href": "29-regularization.html#modeling-movie-effects",
    "title": "29  Regularization",
    "section": "29.4 Modeling movie effects",
    "text": "29.4 Modeling movie effects\n\nWe can use a linear models with a treatment effect \\(b_i\\) for each movie, which can be interpreted as movie effect or the difference between the average ranking for movie \\(i\\) and the overall average \\(\\mu\\):\n\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\n\nStatistics textbooks refer to the \\(b\\)s as treatment effects, however in the Netflix challenge papers they refer to them as bias, thus the \\(b\\) notation.\nWe can again use least squares to estimate the \\(b_i\\) in the following way:\n\n\nfit &lt;- lm(rating ~ as.factor(movieId), data = train_set)\n\n\nBecause there are thousands of \\(b_i\\) as each movie gets one, the lm() function will be very slow here.\nWe don’t recommend running the code above.\nInstead, we leverage the fact that in this particular situation, we know that the least squares estimate \\(\\hat{b}_i\\) is just the average of \\(Y_{u,i} - \\hat{\\mu}\\) for each movie \\(i\\).\nSo we can compute them this way:\n\n\nb_i &lt;- colMeans(y_train - mu, na.rm = TRUE)\n\n\nNote that we drop the hat notation in the code to represent estimates going forward.\nWe can see that these estimates vary substantially:\n\n\nhist(b_i)\n\n\n\n\n\nRemember \\(\\hat{\\mu}=3.5\\) so a \\(b_i = 1.5\\) implies a perfect five star rating.\nLet’s see how much our prediction improves once we use \\(\\hat{y}_{u,i} = \\hat{\\mu} + \\hat{b}_i\\):\n\n\nrmse(sweep(y_test - mu, 2, b_i))\n\n[1] 0.9914404\n\n\n\nWe already see an improvement. But can we make it better?"
  },
  {
    "objectID": "29-regularization.html#user-effects",
    "href": "29-regularization.html#user-effects",
    "title": "29  Regularization",
    "section": "29.5 User effects",
    "text": "29.5 User effects\n\nLet’s compute the average rating for user \\(u\\) for those that have rated 100 or more movies:\n\n\nb_u &lt;- rowMeans(y_train, na.rm = TRUE)\nhist(b_u, nclass = 30)\n\n\n\n\n\nNotice that there is substantial variability across users as well: some users are very cranky and others love most movies.\nThis implies that a further improvement to our model may be:\n\n\\[\nY_{u,i} = \\mu + b_i + b_u + \\varepsilon_{u,i}\n\\]\nwhere \\(b_u\\) is a user-specific effect.\n\nNow if a cranky user (negative \\(b_u\\)) rates a great movie (positive \\(b_i\\)), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.\nTo fit this model, we could again use lm like this:\n\n\nlm(rating ~ as.factor(movieId) + as.factor(userId), data = train_set)\n\n\nHowever, for the reasons described earlier, we won’t run this code.\nInstead, we will compute an approximation by computing \\(\\hat{\\mu}\\) and \\(\\hat{b}_i\\) and estimating \\(\\hat{b}_u\\) as the average of \\(y_{u,i} - \\hat{\\mu} - \\hat{b}_i\\):\n\n\nb_u &lt;- rowMeans(sweep(y_train - mu, 2, b_i), na.rm = TRUE)\n\n\nWe can now construct predictors and see how much the RMSE improves:\n\n\nrmse(sweep(y_test - mu, 2, b_i) - b_u)\n\n[1] 0.910013"
  },
  {
    "objectID": "29-regularization.html#penalized-least-squares",
    "href": "29-regularization.html#penalized-least-squares",
    "title": "29  Regularization",
    "section": "29.6 Penalized least squares",
    "text": "29.6 Penalized least squares\n\nIf we look at the top movies, based on our estimates of the movie effect \\(b_i\\) we find that they are all movies rated three or less times:\n\n\ncolSums(!is.na(y_test[, mu + b_i == 5])) |&gt; table()\n\n\n 1  2  3 \n28  6  3 \n\n\n\nThese are the names of those rated more than three times along with the score in the test set:\n\n\nind &lt;- which(b_i + mu == 5 & colSums(!is.na(y_train)) &gt; 1)\ntitles &lt;- filter(movie_map, movieId %in% colnames(y_train)[ind]) |&gt; pull(title)\nsetNames(colMeans(y_test[,ind], na.rm = TRUE), titles)\n\n Face in the Crowd, A     In a Lonely Place       Pawnbroker, The \n                  5.0                   4.5                   4.0 \n         Mother Night Village of the Damned \n                  4.0                   3.5 \n\n\n\nThese all seem like obscure movies. Do we really think these are the top movies in our database?\nNote that the prediction does not hold on the test set.\nThese supposed best movies were rated by very few users and small sample sizes lead to uncertainty.\nTherefore, larger estimates of \\(b_i\\), negative or positive, are more likely. Therefore, these are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.\nIn previous sections, we computed standard error and constructed confidence intervals to account for different levels of uncertainty.\nHowever, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.\nRegularization permits us to penalize large estimates that are formed using small sample sizes.\nIt has commonalities with the Bayesian approach that shrunk predictions described in the Bayesian models section.\nThe general idea behind regularization is to constrain the total variability of the effect sizes.\nWhy does this help? Consider a case in which we have movie \\(i=1\\) with 100 user ratings and 4 movies \\(i=2,3,4,5\\) with just one user rating. We intend to fit the model\n\n\\[\nY_{u,i} = \\mu + b_i + \\varepsilon_{u,i}\n\\]\n\nSuppose we know the average rating is, say, \\(\\mu = 3\\).\nIf we use least squares, the estimate for the first movie effect \\(b_1\\) is the average of the 100 user ratings, \\(1/100 \\sum_{i=1}^{100} (Y_{i,1} - \\mu)\\), which we expect to be a quite precise.\nHowever, the estimate for movies 2, 3, 4, and 5 will simply be the observed deviation from the average rating \\(\\hat{b}_i = Y_{u,i} - \\hat{\\mu}\\) which is an estimate based on just one number so it won’t be precise at all.\nNote these estimates make the error \\(Y_{u,i} - \\mu + \\hat{b}_i\\) equal to 0 for \\(i=2,3,4,5\\), but we don’t expect to be this lucky next time, when asked to predict.\nIn fact, ignoring the one user and guessing that movies 2,3,4, and 5 are just average movies (\\(b_i = 0\\)) might provide a better prediction.\nThe general idea of penalized regression is to control the total variability of the movie effects: \\(\\sum_{i=1}^5 b_i^2\\).\nSpecifically, instead of minimizing the least squares equation, we minimize an equation that adds a penalty:\n\n\\[\n\\sum_{u,i} \\left(y_{u,i} - \\mu - b_i\\right)^2 + \\lambda \\sum_{i} b_i^2\n\\]\n\nThe first term is just the sum of squares and the second is a penalty that gets larger when many \\(b_i\\)s are large.\nUsing calculus we can actually show that the values of \\(b_i\\) that minimize this equation are:\n\n\\[\n\\hat{b}_i(\\lambda) = \\frac{1}{\\lambda + n_i} \\sum_{u=1}^{n_i} \\left(Y_{u,i} - \\hat{\\mu}\\right)\n\\]\nwhere \\(n_i\\) is the number of ratings made for movie \\(i\\).\n\nThis approach will have our desired effect: when our sample size \\(n_i\\) is very large, a case which will give us a stable estimate, then the penalty \\(\\lambda\\) is effectively ignored since \\(n_i+\\lambda \\approx n_i\\).\nHowever, when the \\(n_i\\) is small, then the estimate \\(\\hat{b}_i(\\lambda)\\) is shrunken towards 0. The larger \\(\\lambda\\), the more we shrink.\nBut how do we select \\(\\lambda\\)? We will later learn about cross-validation which can be used to do this.\nHere we will simply compute the RMSE we for different values of \\(\\lambda\\) to illustrate the effect:\n\n\nn &lt;- colSums(!is.na(y_train))\nsums &lt;- colSums(y_train - mu, na.rm = TRUE)\nlambdas &lt;- seq(0, 10, 0.1)\nrmses &lt;- sapply(lambdas, function(lambda){\n  b_i &lt;-  sums / (n + lambda)\n  rmse(sweep(y_test - mu, 2, b_i))\n})\n\n\nHere is a plot of the RMSE versus \\(\\lambda\\):\n\n\nplot(lambdas, rmses, type = \"l\")\n\n\n\n\n\nThe minimum is obtained for \\(\\lambda=\\) 3.1\nUsing this\\(\\lambda\\) we can compute the regularized estimates and add to our table of estimates:\n\n\nlambda &lt;- lambdas[which.min(rmses)] \nb_i_reg &lt;- colSums(y_train - mu, na.rm = TRUE) / (n + lambda)\n\n\nTo see how the estimates shrink, let’s make a plot of the regularized estimates versus the least squares estimates.\n\n\n\n\n\n\n\nNow, let’s look at the top 5 best movies based on the penalized estimates \\(\\hat{b}_i(\\lambda)\\):\n\n\n\nShawshank Redemption, The            Godfather, The                Roger & Me \n                 4.432836                  4.500000                  3.571429 \n            Thin Man, The        African Queen, The \n                 4.346154                  4.142857 \n\n\n\nThese make much more sense! These movies are watched more and have more ratings in the training set:\n\n\nsetNames(colSums(!is.na(y_test[,ind])), titles)\n\nShawshank Redemption, The            Godfather, The                Roger & Me \n                       67                        37                         7 \n            Thin Man, The        African Queen, The \n                       13                         7 \n\n\n\nWe also see that our RMSE is improved if we use our regularized estimates of the movie effects:\n\n\nb_u &lt;- rowMeans(sweep(y_train - mu, 2, b_i_reg), na.rm = TRUE)\nrmse(sweep(y_test - mu, 2, b_i_reg) - b_u)\n\n[1] 0.8852398\n\n\n\nThe penalized estimates provide an improvement over the least squares estimates:\n\n\n\n\n\n\nmodel\nRMSE\n\n\n\n\nJust the mean\n1.0507764\n\n\nMovie effect\n0.9914404\n\n\nMovie + user effect\n0.9100130\n\n\nRegularized movie + user effect\n0.8852398"
  },
  {
    "objectID": "29-regularization.html#exercises",
    "href": "29-regularization.html#exercises",
    "title": "29  Regularization",
    "section": "29.7 Exercises",
    "text": "29.7 Exercises\n\nFor the movielens data, compute the number of ratings for each movie and then plot it against the year the movie came out. Use the square root transformation on the counts.\nWe see that, on average, movies that came out after 1993 get more ratings. We also see that with newer movies, starting in 1993, the number of ratings decreases with year: the more recent a movie is, the less time users have had to rate it.\n\nAmong movies that came out in 1993 or later, what are the 25 movies with the most ratings per year? Also report their average rating.\n\nFrom the table constructed in the previous example, we see that the most rated movies tend to have above average ratings. This is not surprising: more people watch popular movies. To confirm this, stratify the post 1993 movies by ratings per year and compute their average ratings. Make a plot of average rating versus ratings per year and show an estimate of the trend.\nThe movielens dataset also includes a time stamp. This variable represents the time and data in which the rating was provided. The units are seconds since January 1, 1970. Create a new column date with the date. Hint: use the as_datetime function in the lubridate package.\nCompute the average rating for each week and plot this average against day. Hint: use the round_date function before you group_by.\nThe movielens data also has a genres column. This column includes every genre that applies to the movie. Some movies fall under several genres. Define a category as whatever combination appears in this column. Keep only categories with more than 1,000 ratings. Then compute the average and standard error for each category. Plot these as error bar plots.\nThe plot shows strong evidence of a genre effect. If we define \\(g_{u,i}\\) as the genre for user’s \\(u\\) rating of movie \\(i\\), which of the following models is most appropriate:\n\n\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i} + \\varepsilon_{u,i}\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + d_{u,i}\\beta + \\varepsilon_{u,i}\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + \\sum_{k=1}^K x_{u,i} \\beta_k + \\varepsilon_{u,i}\\), with \\(x^k_{u,i} = 1\\) if \\(g_{u,i}\\) is genre \\(k\\).\n\\(Y_{u,i} = \\mu + b_i + b_u + f(d_{u,i}) + \\varepsilon_{u,i}\\), with \\(f\\) a smooth function of \\(d_{u,i}\\).\n\nAn education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let’s simulate a dataset for 100 schools. First, let’s simulate the number of students in each school.\n\nset.seed(1986)\nn &lt;- round(2^rnorm(1000, 8, 1))\n\nNow let’s assign a true quality for each school completely independent from size. This is the parameter we want to estimate.\n\nmu &lt;- round(80 + 2 * rt(1000, 5))\nrange(mu)\nschools &lt;- data.frame(id = paste(\"PS\",1:100), \n                      size = n, \n                      quality = mu,\n                      rank = rank(-mu))\n\nWe can see that the top 10 schools are:\n\nschools |&gt; top_n(10, quality) |&gt; arrange(desc(quality))\n\nNow let’s have the students in the school take a test. There is random variability in test taking so we will simulate the test scores as normally distributed with the average determined by the school quality and standard deviations of 30 percentage points:\n\nscores &lt;- sapply(1:nrow(schools), function(i){\n  scores &lt;- rnorm(schools$size[i], schools$quality[i], 30)\n  scores\n})\nschools &lt;- schools |&gt; mutate(score = sapply(scores, mean))\n\n\nWhat are the top schools based on the average score? Show just the ID, size, and the average score.\nCompare the median school size to the median school size of the top 10 schools based on the score.\nAccording to this test, it appears small schools are better than large schools. Five out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size are independent. Repeat the exercise for the worst 10 schools.\nThe same is true for the worst schools! They are small as well. Plot the average score versus school size to see what’s going on. Highlight the top 10 schools based on the true quality. Use the log scale transform for the size.\nWe can see that the standard error of the score has larger variability when the school is smaller. This is a basic statistical reality we learned in the probability and inference sections. In fact, note that 4 of the top 10 schools are in the top 10 schools based on the exam score.\n\nLet’s use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. So to apply regularization here, we first need to define the overall average for all schools:\n\noverall &lt;- mean(sapply(scores, mean))\n\nand then define, for each school, how it deviates from that average. Write code that estimates the score above average for each school but dividing by \\(n + \\lambda\\) instead of \\(n\\), with \\(n\\) the school size and \\(\\lambda\\) a regularization parameter. Try \\(\\lambda = 3\\).\n\nNotice that this improves things a bit. The number of small schools that are not highly ranked is now 4. Is there a better \\(\\lambda\\)? Find the \\(\\lambda\\) that minimizes the RMSE = \\(1/100 \\sum_{i=1}^{100} (\\mbox{quality} - \\mbox{estimate})^2\\).\nRank the schools based on the average obtained with the best \\(\\alpha\\). Note that no small school is incorrectly included.\nA common mistake to make when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don’t subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from exercise 6 but without removing the overall mean."
  },
  {
    "objectID": "29-regularization.html#footnotes",
    "href": "29-regularization.html#footnotes",
    "title": "29  Regularization",
    "section": "",
    "text": "http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/↩︎\nhttps://grouplens.org/↩︎"
  },
  {
    "objectID": "30-matrix-factorization.html#sec-factor-analysis",
    "href": "30-matrix-factorization.html#sec-factor-analysis",
    "title": "30  Matrix factorization",
    "section": "30.1 Factor analysis",
    "text": "30.1 Factor analysis\nHere is an illustration, using a simulation, of how we can use some structure to predict the \\(r_{u,i}\\). Suppose our residuals r look like this:\n\nround(r, 1)\n\n   Godfather Godfather2 Goodfellas You've Got Sleepless\n1        2.1        2.5        2.4       -1.6      -1.7\n2        1.9        1.4        2.0       -1.8      -1.3\n3        1.8        2.7        2.3       -2.7      -2.0\n4       -0.5        0.7        0.6       -0.8      -0.5\n5       -0.6       -0.8        0.6        0.4       0.6\n6       -0.1        0.2        0.5       -0.7       0.4\n7       -0.3       -0.1       -0.4       -0.4       0.7\n8        0.3        0.4        0.3        0.0       0.7\n9       -1.4       -2.2       -1.5        2.0       2.8\n10      -2.6       -1.5       -1.3        1.6       1.3\n11      -1.5       -2.0       -2.2        1.7       2.7\n12      -1.5       -1.4       -2.3        2.5       2.0\n\n\nThere seems to be a pattern here. In fact, we can see very strong correlation patterns:\n\ncor(r) \n\n            Godfather Godfather2 Goodfellas You've Got  Sleepless\nGodfather   1.0000000  0.9227480  0.9112020 -0.8976893 -0.8634538\nGodfather2  0.9227480  1.0000000  0.9370237 -0.9498184 -0.9685434\nGoodfellas  0.9112020  0.9370237  1.0000000 -0.9489823 -0.9557691\nYou've Got -0.8976893 -0.9498184 -0.9489823  1.0000000  0.9448660\nSleepless  -0.8634538 -0.9685434 -0.9557691  0.9448660  1.0000000\n\n\nWe can create vectors q and p, that can explain much of the structure we see. The q would look like this:\n\nt(q) \n\n     Godfather Godfather2 Goodfellas You've Got Sleepless\n[1,]         1          1          1         -1        -1\n\n\nand it narrows down movies to two groups: gangster (coded with 1) and romance (coded with -1). We can also reduce the users to three groups:\n\nt(p)\n\n     1 2 3 4 5 6 7 8  9 10 11 12\n[1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2\n\n\nthose that like gangster movies and dislike romance movies (coded as 2), those that like romance movies and dislike gangster movies (coded as -2), and those that don’t care (coded as 0). The main point here is that we can almost reconstruct \\(r\\), which has 60 values, with a couple of vectors totaling 17 values. Note that p and q are equivalent to the patterns and weights we described in Section Section 27.5.\nIf \\(r\\) contains the residuals for users \\(u=1,\\dots,12\\) for movies \\(i=1,\\dots,5\\) we can write the following mathematical formula for our residuals \\(r_{u,i}\\).\n\\[\nr_{u,i} \\approx p_u q_i\n\\]\nThis implies that we can explain more variability by modifying our previous model for movie recommendations to:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_u q_i + \\varepsilon_{u,i}\n\\]\nHowever, we motivated the need for the \\(p_u q_i\\) term with a simple simulation. The structure found in data is usually more complex. For example, in this first simulation we assumed there were was just one factor \\(p_u\\) that determined which of the two genres movie \\(u\\) belongs to. But the structure in our movie data seems to be much more complicated than gangster movie versus romance. We may have many other factors. Here we present a slightly more complex simulation. We now add a sixth movie, Scent of Woman.\n\nround(r, 1)\n\n   Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n1        0.0        0.3        2.2        0.2       0.1  -2.3\n2        2.0        1.7        0.0       -1.9      -1.7   0.3\n3        1.9        2.4        0.1       -2.3      -2.0   0.0\n4       -0.3        0.3        0.3       -0.4      -0.3   0.3\n5       -0.3       -0.4        0.3        0.2       0.3  -0.3\n6        0.9        1.1       -0.8       -1.3      -0.8   1.2\n7        0.9        1.0       -1.2       -1.2      -0.7   0.7\n8        1.2        1.2       -0.9       -1.0      -0.6   0.8\n9       -0.7       -1.1       -0.8        1.0       1.4   0.7\n10      -2.3       -1.8        0.3        1.8       1.7  -0.1\n11      -1.7       -2.0       -0.1        1.9       2.3   0.2\n12      -1.8       -1.7       -0.1        2.3       2.0   0.4\n\n\nBy exploring the correlation structure of this new dataset\n\n\n            Godfather  Godfather2  Goodfellas        YGM          SS\nGodfather   1.0000000  0.97596928 -0.17481747 -0.9729297 -0.95881628\nGodfather2  0.9759693  1.00000000 -0.10510523 -0.9863528 -0.99025965\nGoodfellas -0.1748175 -0.10510523  1.00000000  0.1798809  0.08007665\nYGM        -0.9729297 -0.98635285  0.17988093  1.0000000  0.98675100\nSS         -0.9588163 -0.99025965  0.08007665  0.9867510  1.00000000\nSW          0.1298518  0.08758531 -0.94263256 -0.1632361 -0.08174489\n                    SW\nGodfather   0.12985181\nGodfather2  0.08758531\nGoodfellas -0.94263256\nYGM        -0.16323610\nSS         -0.08174489\nSW          1.00000000\n\n\nwe note that perhaps we need a second factor to account for the fact that some users like Al Pacino, while others dislike him or don’t care. Notice that the overall structure of the correlation obtained from the simulated data is not that far off the real correlation:\n\n\n            Godfather Godfather2  Goodfellas        YGM         SS          SW\nGodfather   1.0000000  0.8326000  0.45833896 -0.3445887 -0.3254261  0.15250174\nGodfather2  0.8326000  1.0000000  0.62626754 -0.2971988 -0.3104670  0.21003950\nGoodfellas  0.4583390  0.6262675  1.00000000 -0.2969603 -0.3904577 -0.07988783\nYGM        -0.3445887 -0.2971988 -0.29696030  1.0000000  0.5306141 -0.21887238\nSS         -0.3254261 -0.3104670 -0.39045775  0.5306141  1.0000000 -0.25664758\nSW          0.1525017  0.2100395 -0.07988783 -0.2188724 -0.2566476  1.00000000\n\n\nTo explain this more complicated structure, we need two factors. For example something like this:\n\nt(q) \n\n     Godfather Godfather2 Goodfellas You've Got Sleepless Scent\n[1,]         1          1          1         -1        -1    -1\n[2,]         1          1         -1         -1        -1     1\n\n\nWith the first factor (the first column of q) used to code the gangster versus romance groups and a second factor (the second column of q) to explain the Al Pacino versus no Al Pacino groups. We will also need two sets of coefficients to explain the variability introduced by the \\(3\\times 3\\) types of groups:\n\nt(p)\n\n      1 2 3 4 5 6 7 8  9 10 11 12\n[1,]  1 1 1 0 0 0 0 0 -1 -1 -1 -1\n[2,] -1 1 1 0 0 1 1 1  0 -1 -1 -1\n\n\nThe model with two factors has 36 parameters that can be used to explain much of the variability in the 72 ratings:\n\\[\nY_{u,i} = \\mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\varepsilon_{u,i}\n\\]\nNote that in an actual data application, we need to fit this model to data. To explain the complex correlation we observe in real data, we usually permit the entries of \\(p\\) and \\(q\\) to be continuous values, rather than discrete ones as we used in the simulation. For example, rather than dividing movies into gangster or romance, we define a continuum. Also note that this is not a linear model and to fit it we need to use an algorithm other than the one used by lm to find the parameters that minimize the least squares. The winning algorithms for the Netflix challenge fit a model similar to the above and used regularization to penalize for large values of \\(p\\) and \\(q\\), rather than using least squares. Implementing this approach is beyond the scope of this book."
  },
  {
    "objectID": "30-matrix-factorization.html#connection-to-svd-and-pca",
    "href": "30-matrix-factorization.html#connection-to-svd-and-pca",
    "title": "30  Matrix factorization",
    "section": "30.2 Connection to SVD and PCA",
    "text": "30.2 Connection to SVD and PCA\nThe decomposition:\n\\[\nr_{u,i} \\approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}\n\\]\nis very much related to SVD and PCA. SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds the vectors \\(p\\) and \\(q\\) that permit us to rewrite the matrix \\(\\mbox{r}\\) with \\(m\\) rows and \\(n\\) columns as:\n\\[\nr_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \\dots + p_{u,n} q_{n,i}\n\\]\nwith the variability of each term decreasing and with the \\(p\\)s uncorrelated. The algorithm also computes this variability so that we can know how much of the matrices, total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability. To illustrate this we will only consider a small subset of movies with many ratings and users that have rated many movies:\n\nlibrary(missMDA)\nind_x &lt;- rowSums(!is.na(r)) &gt;= 250\nind_y &lt;- colSums(!is.na(r[ind_x,])) &gt;= 50 #| colnames(r) %in% keep\n\ny &lt;- y[ind_x, ind_y]\nlambda &lt;- 3.1\nmu &lt;- mean(y, na.rm = TRUE)\nn &lt;- colSums(!is.na(y))\nb_i_reg &lt;- colSums(y - mu, na.rm = TRUE) / (n + lambda)\nb_u &lt;- rowMeans(sweep(y - mu, 2, b_i_reg), na.rm = TRUE)\nr &lt;- sweep(y - mu, 2, b_i_reg) - b_u\ncolnames(r) &lt;- with(movie_map, title[match(colnames(r), movieId)])\n\nimpute &lt;- imputePCA(r, ncp = 10)\npca &lt;- prcomp(impute$completeObs)\n\nThe \\(q\\) vectors are called the principal components and they are stored in this matrix:\n\ndim(pca$rotation)\n\n[1] 136 105\n\n\nWhile the \\(p\\), or the user effects, are here:\n\ndim(pca$x)\n\n[1] 105 105\n\n\nWe can see the variability of each of the vectors:\n\nqplot(1:nrow(pca$x), pca$sdev, xlab = \"PC\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nWe also notice that the first two principal components are related to the structure in opinions about movies:\n\n\n\n\n\nJust by looking at the top 10 in each direction, we see a meaningful patterns. The first PC shows the difference between Hollywood blockbusters on one side:\n\n\n [1] \"780\"  \"4896\" \"1380\" \"7153\" \"1917\" \"7438\" \"4306\" \"6874\" \"586\"  \"736\" \n\n\nand critically acclaimed movies on the other:\n\n\n [1] \"1206\" \"2997\" \"1258\" \"924\"  \"1732\" \"47\"   \"4226\" \"231\"  \"2502\" \"1213\"\n\n\nWhile the second PC seems to be related to nerd favorites or violent movies on one side\n\n\n [1] \"231\"  \"344\"  \"2502\" \"2706\" \"2174\" \"1923\" \"2011\" \"2012\" \"293\"  \"380\" \n\n\nand romantic movies on the other:\n\n\n [1] \"7153\" \"924\"  \"2396\" \"527\"  \"6377\" \"3897\" \"590\"  \"858\"  \"5952\" \"1393\"\n\n\nFitting a model that incorporates these estimates is complicated. For those interested in implementing an approach that incorporates these ideas, we recommend trying the recommenderlab package. The details are beyond the scope of this book."
  },
  {
    "objectID": "30-matrix-factorization.html#exercises",
    "href": "30-matrix-factorization.html#exercises",
    "title": "30  Matrix factorization",
    "section": "30.3 Exercises",
    "text": "30.3 Exercises\nIn this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.\nThe SVD tells us that we can decompose an \\(N\\times p\\) matrix \\(Y\\) with \\(p &lt; N\\) as\n\\[ Y = U D V^{\\top} \\]\nWith \\(U\\) and \\(V\\) orthogonal of dimensions \\(N\\times p\\) and \\(p\\times p\\), respectively, and \\(D\\) a \\(p \\times p\\) diagonal matrix with the values of the diagonal decreasing:\n\\[d_{1,1} \\geq d_{2,2} \\geq \\dots d_{p,p}.\\]\nIn this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:\n\nset.seed(1987)\nn &lt;- 100\nk &lt;- 8\nSigma &lt;- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) \nm &lt;- MASS::mvrnorm(n, rep(0, 3), Sigma)\nm &lt;- m[order(rowMeans(m), decreasing = TRUE),]\ny &lt;- m %x% matrix(rep(1, k), nrow = 1) +\n  matrix(rnorm(matrix(n * k * 3)), n, k * 3)\ncolnames(y) &lt;- c(paste(rep(\"Math\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Science\",k), 1:k, sep=\"_\"),\n                 paste(rep(\"Arts\",k), 1:k, sep=\"_\"))\n\nOur goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just random independent numbers. Are all students just about as good? Does being good in one subject imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this \\(100 \\times 24\\) dataset.\nYou can visualize the 24 test scores for the 100 students by plotting an image:\n\nmy_image &lt;- function(x, zlim = range(x), ...){\n  colors = rev(RColorBrewer::brewer.pal(9, \"RdBu\"))\n  cols &lt;- 1:ncol(x)\n  rows &lt;- 1:nrow(x)\n  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = \"n\", yaxt = \"n\",\n        xlab=\"\", ylab=\"\",  col = colors, zlim = zlim, ...)\n  abline(h=rows + 0.5, v = cols + 0.5)\n  axis(side = 1, cols, colnames(x), las = 2)\n}\n\nmy_image(y)\n\n1. How would you describe the data based on this figure?\n\nThe test scores are all independent of each other.\nThe students that test well are at the top of the image and there seem to be three groupings by subject.\nThe students that are good at math are not good at science.\nThe students that are good at math are not good at humanities.\n\n2. You can examine the correlation between the test scores directly like this:\n\nmy_image(cor(y), zlim = c(-1,1))\nrange(cor(y))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWhich of the following best describes what you see?\n\nThe test scores are independent.\nMath and science are highly correlated but the humanities are not.\nThere is high correlation between tests in the same subject but no correlation across subjects.\nThere is a correlation among all tests, but higher if the tests are in science and math and even higher within each subject.\n\n3. Remember that orthogonality means that \\(U^{\\top}U\\) and \\(V^{\\top}V\\) are equal to the identity matrix. This implies that we can also rewrite the decomposition as\n\\[ Y V = U D \\mbox{ or } U^{\\top}Y = D V^{\\top}\\]\nWe can think of \\(YV\\) and \\(U^{\\top}V\\) as two transformations of Y that preserve the total variability of \\(Y\\) since \\(U\\) and \\(V\\) are orthogonal.\nUse the function svd to compute the SVD of y. This function will return \\(U\\), \\(V\\) and the diagonal entries of \\(D\\).\n\ns &lt;- svd(y)\nnames(s)\n\nYou can check that the SVD works by typing:\n\ny_svd &lt;- s$u %*% diag(s$d) %*% t(s$v)\nmax(abs(y - y_svd))\n\nCompute the sum of squares of the columns of \\(Y\\) and store them in ss_y. Then compute the sum of squares of columns of the transformed \\(YV\\) and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).\n4. We see that the total sum of squares is preserved. This is because \\(V\\) is orthogonal. Now to start understanding how \\(YV\\) is useful, plot ss_y against the column number and then do the same for ss_yv. What do you observe?\n5. We see that the variability of the columns of \\(YV\\) is decreasing. Furthermore, we see that, relative to the first three, the variability of the columns beyond the third is almost 0. Now notice that we didn’t have to compute ss_yv because we already have the answer. How? Remember that \\(YV = UD\\) and because \\(U\\) is orthogonal, we know that the sum of squares of the columns of \\(UD\\) are the diagonal entries of \\(D\\) squared. Confirm this by plotting the square root of ss_yv versus the diagonal entries of \\(D\\).\n6. From the above we know that the sum of squares of the columns of \\(Y\\) (the total sum of squares) add up to the sum of s$d^2 and that the transformation \\(YV\\) gives us columns with sums of squares equal to s$d^2. Now compute what percent of the total variability is explained by just the first three columns of \\(YV\\).\n7. We see that almost 99% of the variability is explained by the first three columns of \\(YV = UD\\). So we get the sense that we should be able to explain much of the variability and structure we found while exploring the data with a few columns. Before we continue, let’s show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write \\(U\\) out in its columns \\([U_1, U_2, \\dots, U_p]\\) then \\(UD\\) is equal to\n\\[UD = [U_1 d_{1,1}, U_2 d_{2,2}, \\dots, U_p d_{p,p}]\\]\nUse the sweep function to compute \\(UD\\) without constructing diag(s$d) nor matrix multiplication.\n8. We know that \\(U_1 d_{1,1}\\), the first column of \\(UD\\), has the most variability of all the columns of \\(UD\\). Earlier we saw an image of \\(Y\\):\n\nmy_image(y)\n\nin which we can see that the student to student variability is quite large and that it appears that students that are good in one subject are good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student and plot it against \\(U_1 d_{1,1}\\) and describe what you find.\n9. We note that the signs in SVD are arbitrary because:\n\\[ U D V^{\\top} = (-U) D (-V)^{\\top} \\]\nWith this in mind we see that the first column of \\(UD\\) is almost identical to the average score for each student except for the sign.\nThis implies that multiplying \\(Y\\) by the first column of \\(V\\) must be performing a similar operation to taking the average. Make an image plot of \\(V\\) and describe the first column relative to others and how this relates to taking an average.\n10. We already saw that we can rewrite \\(UD\\) as\n\\[U_1 d_{1,1} + U_2 d_{2,2} + \\dots + U_p d_{p,p}\\]\nwith \\(U_j\\) the j-th column of \\(U\\). This implies that we can rewrite the entire SVD as:\n\\[Y = U_1 d_{1,1} V_1 ^{\\top} + U_2 d_{2,2} V_2 ^{\\top} + \\dots + U_p d_{p,p} V_p ^{\\top}\\]\nwith \\(V_j\\) the jth column of \\(V\\). Plot \\(U_1\\), then plot \\(V_1^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_1 d_{1,1} V_1 ^{\\top}\\) and compare it to the image of \\(Y\\). Hint: use the my_image function defined above and use the drop=FALSE argument to assure the subsets of matrices are matrices.\n11. We see that with just a vector of length 100, a scalar, and a vector of length 24, we actually come close to reconstructing the original \\(100 \\times 24\\) matrix. This is our first matrix factorization:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top}\\] We know it explains s$d[1]^2/sum(s$d^2) * 100 percent of the total variability. Our approximation only explains the observation that good students tend to be good in all subjects. But another aspect of the original data that our approximation does not explain was the higher similarity we observed within subjects. We can see this by computing the difference between our approximation and original data and then computing the correlations. You can see this by running this code:\n\nresid &lt;- y - with(s,(u[,1, drop=FALSE]*d[1]) %*% t(v[,1, drop=FALSE]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nNow that we have removed the overall student effect, the correlation plot reveals that we have not yet explained the within subject correlation nor the fact that math and science are closer to each other than to the arts. So let’s explore the second column of the SVD. Repeat the previous exercise but for the second column: Plot \\(U_2\\), then plot \\(V_2^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_2 d_{2,2} V_2 ^{\\top}\\) and compare it to the image of resid.\n12. The second column clearly relates to a student’s difference in ability in math/science versus the arts. We can see this most clearly from the plot of s$v[,2]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} \\]\nWe know it will explain\n\nsum(s$d[1:2]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:2], 2, d[1:2], FUN=\"*\") %*% t(v[,1:2]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nand see that the structure that is left is driven by the differences between math and science. Confirm this by plotting \\(U_3\\), then plot \\(V_3^{\\top}\\) using the same range for the y-axis limits, then make an image of \\(U_3 d_{3,3} V_3 ^{\\top}\\) and compare it to the image of resid.\n13. The third column clearly relates to a student’s difference in ability in math and science. We can see this most clearly from the plot of s$v[,3]. Adding the matrix we obtain with these two columns will help with our approximation:\n\\[ Y \\approx d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\]\nWe know it will explain:\n\nsum(s$d[1:3]^2)/sum(s$d^2) * 100\n\npercent of the total variability. We can compute new residuals like this:\n\nresid &lt;- y - with(s,sweep(u[,1:3], 2, d[1:3], FUN=\"*\") %*% t(v[,1:3]))\nmy_image(cor(resid), zlim = c(-1,1))\naxis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)\n\nWe no longer see structure in the residuals: they seem to be independent of each other. This implies that we can describe the data with the following model:\n\\[ Y =  d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top} + \\varepsilon\\]\nwith \\(\\varepsilon\\) a matrix of independent identically distributed errors. This model is useful because we summarize of \\(100 \\times 24\\) observations with \\(3 \\times (100+24+1) = 375\\) numbers. Furthermore, the three components of the model have useful interpretations: 1) the overall ability of a student, 2) the difference in ability between the math/sciences and arts, and 3) the remaining differences between the three subjects. The sizes \\(d_{1,1}, d_{2,2}\\) and \\(d_{3,3}\\) tell us the variability explained by each component. Finally, note that the components \\(d_{j,j} U_j V_j^{\\top}\\) are equivalent to the jth principal component.\nFinish the exercise by plotting an image of \\(Y\\), an image of \\(d_{1,1} U_1 V_1^{\\top} + d_{2,2} U_2 V_2^{\\top} + d_{3,3} U_3 V_3^{\\top}\\) and an image of the residuals, all with the same zlim.\n14. Advanced. The movielens dataset included in the dslabs package is a small subset of a larger dataset with millions of ratings. You can find the entire latest dataset here https://grouplens.org/datasets/movielens/20m/. Create your own recommendation system using all the tools we have shown you."
  }
]